"""
ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸

í˜„ì‹¤ì ì¸ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¨¼ì € ì‹¤í–‰í•œ í›„ train_gru.pyë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

ì—…ë°ì´íŠ¸:
- ë‹¤ì¤‘ ì‹œë“œ ì§€ì› (ë‹¤ì–‘ì„± ì¦ê°€)
- ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ìƒì„± (2000ì¼+)
- ì˜¤ì—¼ ë°œìƒ/ë¯¸ë°œìƒ ë¼ë²¨ë§
"""

import os
import sys
import numpy as np

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.dataset.realistic_dataset_generator import RealisticRoutineGenerator
from training.config import PATHS


def compute_pollution_occurred_labels(y: np.ndarray) -> np.ndarray:
    """
    ì˜¤ì—¼ ë°œìƒ ì—¬ë¶€ ë¼ë²¨ ê³„ì‚°

    ì´ì „ íƒ€ì„ìŠ¤í… ëŒ€ë¹„ ì˜¤ì—¼ë„ê°€ ì¦ê°€í–ˆìœ¼ë©´ 1, ì•„ë‹ˆë©´ 0
    """
    pollution_occurred = np.zeros(len(y), dtype=np.float32)

    for i in range(1, len(y)):
        # 4ê°œ êµ¬ì—­ ì¤‘ í•˜ë‚˜ë¼ë„ ì˜¤ì—¼ë„ê°€ ì¦ê°€í–ˆìœ¼ë©´ 1
        if np.any(y[i] > y[i-1]):
            pollution_occurred[i] = 1.0

    # ì²« ë²ˆì§¸ íƒ€ì„ìŠ¤í…ì€ ì´ˆê¸° ì˜¤ì—¼ë„(0.1)ë³´ë‹¤ ë†’ìœ¼ë©´ 1
    if len(y) > 0 and np.any(y[0] > 0.1):
        pollution_occurred[0] = 1.0

    return pollution_occurred


def generate_multi_seed_dataset(num_days: int, num_seeds: int, timesteps_per_hour: int = 4):
    """
    ì—¬ëŸ¬ ì‹œë“œë¡œ ë°ì´í„° ìƒì„±í•˜ì—¬ ë‹¤ì–‘ì„± í™•ë³´

    Args:
        num_days: ê° ì‹œë“œë‹¹ ìƒì„±í•  ë‚ ì§œ ìˆ˜
        num_seeds: ì‚¬ìš©í•  ì‹œë“œ ê°œìˆ˜
        timesteps_per_hour: ì‹œê°„ë‹¹ íƒ€ì„ìŠ¤í… ìˆ˜

    Returns:
        í†µí•©ëœ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹
    """
    print("\n" + "=" * 80)
    print("ëŒ€ê·œëª¨ ë‹¤ì¤‘ ì‹œë“œ ë°ì´í„°ì…‹ ìƒì„±")
    print("=" * 80)
    print(f"ì„¤ì •:")
    print(f"  - ì‹œë“œë‹¹ ë‚ ì§œ ìˆ˜: {num_days}ì¼")
    print(f"  - ì‹œë“œ ê°œìˆ˜: {num_seeds}ê°œ")
    print(f"  - ì‹œê°„ë‹¹ íƒ€ì„ìŠ¤í…: {timesteps_per_hour}ê°œ (15ë¶„ ê°„ê²©)")
    print(f"  - ì˜ˆìƒ ì´ íƒ€ì„ìŠ¤í…: ~{num_days * num_seeds * 24 * timesteps_per_hour * 1.15:,.0f}ê°œ")
    print(f"    (ìë™ ì²­ì†Œ íŠ¸ë¦¬ê±° í¬í•¨)")
    print("=" * 80 + "\n")

    # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ í†µí•© ë¦¬ìŠ¤íŠ¸
    all_data = {
        'time': [],
        'spatial': [],
        'visual': [],
        'audio': [],
        'pose': [],
        'y': [],
        'pollution_occurred': []  # ìƒˆë¡œìš´ ë¼ë²¨: ì˜¤ì—¼ ë°œìƒ ì—¬ë¶€
    }

    # ê° ì‹œë“œë³„ë¡œ ë°ì´í„° ìƒì„±
    for seed_idx in range(num_seeds):
        seed = 42 + seed_idx  # 42, 43, 44, ...

        print(f"\n{'â”€' * 80}")
        print(f"ì‹œë“œ {seed_idx + 1}/{num_seeds} (seed={seed}) ìƒì„± ì¤‘...")
        print(f"{'â”€' * 80}")

        generator = RealisticRoutineGenerator(seed=seed)

        dataset = generator.generate_dataset(
            num_days=num_days,
            timesteps_per_hour=timesteps_per_hour,
            output_path=None  # ì„ì‹œë¡œ ì €ì¥ ì•ˆ í•¨
        )

        # ë°ì´í„° í†µí•©
        for key in ['time', 'spatial', 'visual', 'audio', 'pose', 'y']:
            all_data[key].append(dataset[key])

        # ì˜¤ì—¼ ë°œìƒ ë¼ë²¨ ê³„ì‚°
        pollution_labels = compute_pollution_occurred_labels(dataset['y'])
        all_data['pollution_occurred'].append(pollution_labels)

        print(f"âœ“ ì‹œë“œ {seed} ì™„ë£Œ: {len(dataset['y']):,} íƒ€ì„ìŠ¤í…")

    # ëª¨ë“  ì‹œë“œ ë°ì´í„° í†µí•©
    print(f"\n{'=' * 80}")
    print("ëª¨ë“  ì‹œë“œ ë°ì´í„° ë³‘í•© ì¤‘...")
    print(f"{'=' * 80}")

    merged_dataset = {}
    for key in ['time', 'spatial', 'visual', 'audio', 'pose', 'y', 'pollution_occurred']:
        merged_dataset[key] = np.concatenate(all_data[key], axis=0)
        print(f"  {key:20s}: {merged_dataset[key].shape}")

    # ë©”íƒ€ë°ì´í„°
    merged_dataset['metadata'] = {
        'num_days_per_seed': num_days,
        'num_seeds': num_seeds,
        'timesteps_per_hour': timesteps_per_hour,
        'total_timesteps': len(merged_dataset['y']),
        'total_days_equivalent': num_days * num_seeds,
        'seeds_used': list(range(42, 42 + num_seeds)),
        'num_zones': 4,
        'zones': ['balcony', 'bedroom', 'kitchen', 'living_room'],
        'feature_dims': {
            'time': 10,
            'spatial': 4,
            'visual': 14,
            'audio': 17,
            'pose': 51
        },
        'new_labels': {
            'pollution_occurred': 'ì˜¤ì—¼ ë°œìƒ ì—¬ë¶€ (1: ë°œìƒ, 0: ë¯¸ë°œìƒ)'
        }
    }

    # í†µê³„ ì¶œë ¥
    print(f"\n{'=' * 80}")
    print("ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print(f"{'=' * 80}")
    print(f"ì´ íƒ€ì„ìŠ¤í…: {merged_dataset['metadata']['total_timesteps']:,}")
    print(f"ë“±ê°€ ì¼ìˆ˜: {merged_dataset['metadata']['total_days_equivalent']:,}ì¼")

    # ì˜¤ì—¼ ë°œìƒ í†µê³„
    y = merged_dataset['y']
    pollution_occurred = merged_dataset['pollution_occurred']

    print(f"\n{'â”€' * 80}")
    print("êµ¬ì—­ë³„ ì˜¤ì—¼ë„ í†µê³„")
    print(f"{'â”€' * 80}")
    zones = ['balcony', 'bedroom', 'kitchen', 'living_room']
    for i, zone in enumerate(zones):
        print(f"  {zone:15s}: mean={y[:, i].mean():.3f}, std={y[:, i].std():.3f}, "
              f"min={y[:, i].min():.3f}, max={y[:, i].max():.3f}")

    print(f"\n{'â”€' * 80}")
    print("ì˜¤ì—¼ ë°œìƒ ì—¬ë¶€ í†µê³„")
    print(f"{'â”€' * 80}")
    num_polluted = int(pollution_occurred.sum())
    num_clean = len(pollution_occurred) - num_polluted
    print(f"  ì˜¤ì—¼ ë°œìƒ:     {num_polluted:,} íƒ€ì„ìŠ¤í… ({num_polluted/len(pollution_occurred)*100:.1f}%)")
    print(f"  ì˜¤ì—¼ ë¯¸ë°œìƒ:   {num_clean:,} íƒ€ì„ìŠ¤í… ({num_clean/len(pollution_occurred)*100:.1f}%)")
    print(f"  í•©ê³„:          {len(pollution_occurred):,} íƒ€ì„ìŠ¤í…")

    return merged_dataset


def main():
    """í˜„ì‹¤ì ì¸ ì„¼ì„œ ë°ì´í„° ìƒì„± ë° ì €ì¥"""
    print("\n" + "â•”" + "â•" * 78 + "â•—")
    print("â•‘" + " " * 25 + "ë°ì´í„° ì¤€ë¹„ ë„êµ¬" + " " * 38 + "â•‘")
    print("â•š" + "â•" * 78 + "â•")

    print("\në°ì´í„°ì…‹ ìƒì„± ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("  1. ê¸°ë³¸ ë°ì´í„°ì…‹ (500ì¼, 1ê°œ ì‹œë“œ) - ë¹ ë¦„")
    print("  2. ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ (2000ì¼, 3ê°œ ì‹œë“œ) - ì¶”ì²œ")
    print("  3. ì´ˆëŒ€ê·œëª¨ ë°ì´í„°ì…‹ (3000ì¼, 5ê°œ ì‹œë“œ) - ìµœëŒ€")
    print("  4. ì»¤ìŠ¤í…€ ì„¤ì •")

    # í™˜ê²½ ë³€ìˆ˜ë‚˜ íŒŒì´í”„ ì…ë ¥ ì§€ì›
    import sys
    if not sys.stdin.isatty():
        choice = sys.stdin.readline().strip()
    else:
        choice = input("\nì„ íƒ (1-4): ").strip()

    if choice == '1':
        num_days = 500
        num_seeds = 1
        output_filename = 'realistic_training_dataset.npz'
    elif choice == '2':
        num_days = 2000
        num_seeds = 3
        output_filename = 'massive_dataset_2000days_3seeds.npz'
    elif choice == '3':
        num_days = 3000
        num_seeds = 5
        output_filename = 'massive_dataset_3000days_5seeds.npz'
    elif choice == '4':
        num_days = int(input("ë‚ ì§œ ìˆ˜ (ì‹œë“œë‹¹): "))
        num_seeds = int(input("ì‹œë“œ ê°œìˆ˜: "))
        output_filename = f'custom_dataset_{num_days}days_{num_seeds}seeds.npz'
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê¸°ë³¸ê°’(ì˜µì…˜ 1)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        num_days = 500
        num_seeds = 1
        output_filename = 'realistic_training_dataset.npz'

    # ê¸°ì¡´ ë°ì´í„° í™•ì¸
    output_path = os.path.join(PATHS['data_dir'], output_filename)

    if os.path.exists(output_path):
        print(f"\nâš ï¸  ê¸°ì¡´ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤: {output_path}")
        user_input = input("ë®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")

        if user_input.lower() != 'y':
            print("ë°ì´í„° ìƒì„±ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
            print(f"ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ train_gru.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return

    # ë°ì´í„° ìƒì„±
    if num_seeds == 1:
        # ë‹¨ì¼ ì‹œë“œ (ê¸°ì¡´ ë°©ì‹)
        print("\nê¸°ë³¸ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        generator = RealisticRoutineGenerator(seed=42)
        dataset = generator.generate_dataset(
            num_days=num_days,
            timesteps_per_hour=4,
            output_path=None
        )

        # ì˜¤ì—¼ ë°œìƒ ë¼ë²¨ ì¶”ê°€
        dataset['pollution_occurred'] = compute_pollution_occurred_labels(dataset['y'])

    else:
        # ë‹¤ì¤‘ ì‹œë“œ
        dataset = generate_multi_seed_dataset(
            num_days=num_days,
            num_seeds=num_seeds,
            timesteps_per_hour=4
        )

    # ì €ì¥
    print(f"\n{'=' * 80}")
    print("ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...")
    print(f"{'=' * 80}")
    print(f"ê²½ë¡œ: {output_path}")

    np.savez_compressed(output_path, **dataset)

    file_size_mb = os.path.getsize(output_path) / 1024 / 1024

    print("\n" + "=" * 80)
    print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nì €ì¥ëœ íŒŒì¼:")
    print(f"  ğŸ“ {output_path}")
    print(f"  ğŸ“Š ì´ íƒ€ì„ìŠ¤í…: {len(dataset['y']):,}")
    print(f"  ğŸ’¾ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
    print(f"  ğŸ—œï¸  ì••ì¶•ë¥ : ~{len(dataset['y']) * 96 * 4 / 1024 / 1024 / file_size_mb:.1f}x")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. python training/train_encoder.py  # AttentionEncoder í•™ìŠµ (Base Layer)")
    print(f"  2. python training/train_gru.py      # GRU í•™ìŠµ (Head Layer)")


if __name__ == "__main__":
    main()
